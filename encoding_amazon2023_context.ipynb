{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Item Textual and Visual Modality Features for Amazon2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable\n",
    "\n",
    "import torch, tqdm, os\n",
    "import pandas as pd\n",
    "from freerec.data.tags import USER, ITEM\n",
    "from freerec.data.utils import download_from_url\n",
    "from freerec.utils import export_pickle\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torchdata.datapipes as dp\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the models according to [[hf-mirror](https://hf-mirror.com/)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: str = \"Amazon2023Beauty_10104811_ROU\"\n",
    "datadir: str = f\"../data/Processed/{dataset}\"\n",
    "image_folder: str = os.path.join(datadir, \"item_images\", \"large\")\n",
    "model_cache_dir: str = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>IMAGE_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Klutz Metallic Glam Nail Studio Activit...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51dyKdZMlC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Title: Versace Bright Crystal Eau de Toilette ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41lnN8CpvE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Title: Conair CD82ZCS Instant Heat Curling Iro...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31N529CJ78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Title: Conair CD82ZCS Instant Heat Curling Iro...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31N529CJ78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Title: Refill Cartridges CCR\\nFeatures: 1. Cle...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41KIM5M9xi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ITEM                                               TEXT  \\\n",
       "0     0  Title: Klutz Metallic Glam Nail Studio Activit...   \n",
       "1     1  Title: Versace Bright Crystal Eau de Toilette ...   \n",
       "2     2  Title: Conair CD82ZCS Instant Heat Curling Iro...   \n",
       "3     3  Title: Conair CD82ZCS Instant Heat Curling Iro...   \n",
       "4     4  Title: Refill Cartridges CCR\\nFeatures: 1. Cle...   \n",
       "\n",
       "                                           IMAGE_URL  \n",
       "0  https://m.media-amazon.com/images/I/51dyKdZMlC...  \n",
       "1  https://m.media-amazon.com/images/I/41lnN8CpvE...  \n",
       "2  https://m.media-amazon.com/images/I/31N529CJ78...  \n",
       "3  https://m.media-amazon.com/images/I/31N529CJ78...  \n",
       "4  https://m.media-amazon.com/images/I/41KIM5M9xi...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df = pd.read_csv(\n",
    "    os.path.join(datadir, \"item.txt\"), sep='\\t'\n",
    ")\n",
    "item_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download images from given urls. Please check `image_size` before going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(item_df: pd.DataFrame):\n",
    "    ids = item_df[ITEM.name]\n",
    "    urls = item_df['IMAGE_URL']\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for id_, url in tqdm.tqdm(zip(ids, urls), desc=\"Download images: \"):\n",
    "            if url:\n",
    "                executor.submit(\n",
    "                    download_from_url,\n",
    "                    url=url,\n",
    "                    root=image_folder,\n",
    "                    filename=f\"{id_}.jpg\",\n",
    "                    log=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download images: : 30380it [00:32, 937.29it/s] \n"
     ]
    }
   ],
   "source": [
    "download_images(item_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will encode visual modality first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_visual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    model: str, model_dir: str,\n",
    "    num_workers: int = 4, batch_size: int = 128,\n",
    "):\n",
    "    images = []\n",
    "    try:\n",
    "        processor = AutoImageProcessor.from_pretrained(\n",
    "            os.path.join(model_dir, model), local_files_only=True\n",
    "        )\n",
    "    except OSError:\n",
    "        print(\"No processor file\")\n",
    "        processor = lambda x: x\n",
    "\n",
    "    is_missed_urls = torch.ones((len(item_df,)))\n",
    "    def _process(idx: int):\n",
    "        try:\n",
    "            image = Image.open(\n",
    "                os.path.join(\n",
    "                    image_folder, f\"{idx}.jpg\"\n",
    "                )\n",
    "            ).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            is_missed_urls[idx] = 0\n",
    "        return idx, processor(images=image, return_tensors='pt')['pixel_values'][0]\n",
    "\n",
    "    datapipe = dp.iter.IterableWrapper(\n",
    "        range(len(item_df))\n",
    "    ).sharding_filter().map(\n",
    "        _process\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        datapipe, \n",
    "        num_workers=num_workers, batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = AutoModel.from_pretrained(\n",
    "        os.path.join(model_dir, model), local_files_only=True\n",
    "    ).to(device).eval()\n",
    "\n",
    "    vIndices = []\n",
    "    vFeats = []\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        for (indices, images) in tqdm.tqdm(dataloader, desc=\"Visual batches: \"):\n",
    "            vIndices.append(indices)\n",
    "            outputs = encoder(pixel_values=images.to(device)).last_hidden_state\n",
    "            if outputs.ndim == 3:\n",
    "                # vit (Batch, Sequence, D) -> (Batch, D, Sequence)\n",
    "                outputs = outputs.transpose(-1, -2)\n",
    "            else:\n",
    "                # resnet (Batch, D, K, K) -> (Batch, D, K x K)\n",
    "                outputs = outputs.flatten(2)\n",
    "            vFeats.append(\n",
    "                outputs.mean(-1)\n",
    "            )\n",
    "    vIndices = torch.cat(vIndices, dim=0)\n",
    "    vFeats = torch.cat(vFeats, dim=0).flatten(1) # (N, D)\n",
    "    vFeats = vFeats[vIndices.argsort()] # reindex\n",
    "    assert vFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    is_missed_urls = is_missed_urls.bool().to(vFeats.device)\n",
    "    mean = vFeats[is_missed_urls].mean(dim=0, keepdim=True).repeat((vFeats.size(0), 1))\n",
    "    is_missed_urls = is_missed_urls.unsqueeze(-1).expand_as(vFeats)\n",
    "    vFeats = torch.where(is_missed_urls, vFeats, mean)\n",
    "\n",
    "    export_pickle(\n",
    "        vFeats, os.path.join(\n",
    "            datadir, f\"visual_{model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return vFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxu/miniconda3/lib/python3.9/site-packages/torch/utils/data/datapipes/utils/common.py:142: UserWarning: Local function is not supported by pickle, please use regular python function or functools.partial instead.\n",
      "  warnings.warn(\n",
      "Visual batches:   0%|          | 0/238 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Visual batches: : 240it [03:41,  1.08it/s]                       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0488, -0.0903, -0.2875,  ..., -0.0211, -0.2395,  0.0547],\n",
       "        [ 0.2017,  0.0873, -0.1558,  ..., -0.1068,  0.2998,  0.1285],\n",
       "        [-0.0352,  0.2041,  0.0408,  ..., -0.0813,  0.0746,  0.0412],\n",
       "        ...,\n",
       "        [-0.1274,  0.1027, -0.1186,  ..., -0.0758, -0.0648, -0.0265],\n",
       "        [ 0.1171, -0.1759,  0.0610,  ...,  0.0516, -0.0141, -0.1135],\n",
       "        [-0.2421,  0.3244, -0.1851,  ..., -0.2860,  0.1293,  0.0326]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_visual_modality(\n",
    "    item_df,\n",
    "    model=\"vit-base-16-224\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_textual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    model: str, model_dir: str,\n",
    "    batch_size: int = 128\n",
    "):\n",
    "    sentences = item_df['TEXT']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, model),\n",
    "        device=device\n",
    "    ).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tFeats = encoder.encode(\n",
    "            sentences, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "    assert tFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    export_pickle(\n",
    "        tFeats, os.path.join(\n",
    "            datadir, f\"textual_{model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return tFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 238/238 [03:22<00:00,  1.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2388, -0.1938, -0.3167,  ..., -0.0088, -0.0606, -0.3454],\n",
       "        [ 0.5566, -0.7622,  0.1914,  ...,  0.5741,  0.0815,  0.2836],\n",
       "        [ 0.7353,  0.8229, -0.2983,  ...,  0.1093, -0.0770, -0.1295],\n",
       "        ...,\n",
       "        [ 1.1915, -0.4700,  0.2279,  ..., -1.0194,  0.5871, -0.0360],\n",
       "        [ 0.4231, -0.4110, -0.7817,  ..., -0.7333,  0.1331, -0.4086],\n",
       "        [ 1.5562,  0.1625,  0.5239,  ..., -1.0524,  0.3007,  0.2076]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_textual_modality(\n",
    "    item_df,\n",
    "    model=\"roberta-base\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to extract features by CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_clip_textual_visual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    img_clip_model: str,\n",
    "    text_clip_model: str,\n",
    "    model_dir: str,\n",
    "    batch_size: int = 128\n",
    "):\n",
    "    images = []\n",
    "    is_missed_urls = torch.ones((len(item_df,)))\n",
    "    for idx in range(len(item_df)):\n",
    "        try:\n",
    "            image = Image.open(\n",
    "                os.path.join(\n",
    "                    image_folder, f\"{idx}.jpg\"\n",
    "                )\n",
    "            ).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            is_missed_urls[idx] = 0\n",
    "        images.append(image)\n",
    "\n",
    "    sentences = item_df['TEXT']\n",
    "\n",
    "    img_encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, img_clip_model),\n",
    "        device=torch.device('cuda:0')\n",
    "    ).eval()\n",
    "    text_encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, text_clip_model),\n",
    "        device=torch.device('cuda:1')\n",
    "    ).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vFeats = img_encoder.encode(\n",
    "            images,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "        tFeats = text_encoder.encode(\n",
    "            sentences, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "    assert vFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "    assert tFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    is_missed_urls = is_missed_urls.bool().to(vFeats.device)\n",
    "    mean = vFeats[is_missed_urls].mean(dim=0, keepdim=True).repeat((vFeats.size(0), 1))\n",
    "    is_missed_urls = is_missed_urls.unsqueeze(-1).expand_as(vFeats)\n",
    "    vFeats = torch.where(is_missed_urls, vFeats, mean)\n",
    "\n",
    "    export_pickle(\n",
    "        vFeats, os.path.join(\n",
    "            datadir, f\"visual_{img_clip_model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    export_pickle(\n",
    "        tFeats, os.path.join(\n",
    "            datadir, f\"textual_{text_clip_model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return vFeats, tFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 238/238 [04:12<00:00,  1.06s/it]\n",
      "Batches: 100%|██████████| 238/238 [01:41<00:00,  2.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0367, -0.4885,  0.3980,  ..., -0.1649,  0.4623,  0.0621],\n",
       "         [-0.2082, -0.0440,  0.0904,  ...,  0.3867, -0.0183, -0.0969],\n",
       "         [-0.0832,  0.3915, -0.0901,  ...,  0.9376,  0.2346,  0.1945],\n",
       "         ...,\n",
       "         [-0.1495,  0.3978,  0.2217,  ...,  0.1837, -0.4292,  0.1551],\n",
       "         [-0.2783,  0.1620,  0.1765,  ...,  0.4337, -0.3054, -0.1246],\n",
       "         [-0.2874, -0.1172,  0.1805,  ...,  0.1124,  0.0043,  0.2117]]),\n",
       " tensor([[-0.0480,  0.1438, -0.0667,  ...,  0.0107,  0.0527, -0.1216],\n",
       "         [ 0.0211,  0.1272, -0.0593,  ...,  0.0686,  0.0270, -0.0941],\n",
       "         [-0.0333,  0.1581, -0.0986,  ...,  0.0823,  0.0271,  0.0128],\n",
       "         ...,\n",
       "         [ 0.0293,  0.1610, -0.0843,  ..., -0.0547, -0.0385, -0.0440],\n",
       "         [ 0.1392,  0.0145, -0.0654,  ..., -0.0097,  0.0556, -0.1122],\n",
       "         [ 0.0427,  0.1206, -0.1094,  ...,  0.0034, -0.0012, -0.0377]]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_clip_textual_visual_modality(\n",
    "    item_df,\n",
    "    img_clip_model=\"clip-vit-b-32\",\n",
    "    text_clip_model=\"clip-vit-b-32-multilingual-v1\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bbd328b7770ce4cdf5c7e0c541d9983090dbc8cb119d50c9ca454511f96043d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
