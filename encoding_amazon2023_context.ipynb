{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Item Textual and Visual Modality Features for Amazon2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxu/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Iterable\n",
    "\n",
    "import torch, tqdm, os\n",
    "import pandas as pd\n",
    "from freerec.data.tags import USER, ITEM\n",
    "from freerec.data.utils import download_from_url\n",
    "from freerec.utils import export_pickle\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torchdata.datapipes as dp\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the models according to [[hf-mirror](https://hf-mirror.com/)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: str = \"Amazon2023Beauty_10104811_ROU\"\n",
    "datadir: str = f\"../data/Processed/{dataset}\"\n",
    "image_folder: str = os.path.join(datadir, \"item_images\", \"large\")\n",
    "model_cache_dir: str = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>IMAGE_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Klutz Metallic Glam Nail Studio Activit...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/51dyKdZMlC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Title: Versace Bright Crystal Eau de Toilette ...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41lnN8CpvE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Title: Conair CD82ZCS Instant Heat Curling Iro...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31N529CJ78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Title: Conair CD82ZCS Instant Heat Curling Iro...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/31N529CJ78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Title: Refill Cartridges CCR\\nFeatures: 1. Cle...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41KIM5M9xi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ITEM                                               TEXT  \\\n",
       "0     0  Title: Klutz Metallic Glam Nail Studio Activit...   \n",
       "1     1  Title: Versace Bright Crystal Eau de Toilette ...   \n",
       "2     2  Title: Conair CD82ZCS Instant Heat Curling Iro...   \n",
       "3     3  Title: Conair CD82ZCS Instant Heat Curling Iro...   \n",
       "4     4  Title: Refill Cartridges CCR\\nFeatures: 1. Cle...   \n",
       "\n",
       "                                           IMAGE_URL  \n",
       "0  https://m.media-amazon.com/images/I/51dyKdZMlC...  \n",
       "1  https://m.media-amazon.com/images/I/41lnN8CpvE...  \n",
       "2  https://m.media-amazon.com/images/I/31N529CJ78...  \n",
       "3  https://m.media-amazon.com/images/I/31N529CJ78...  \n",
       "4  https://m.media-amazon.com/images/I/41KIM5M9xi...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df = pd.read_csv(\n",
    "    os.path.join(datadir, \"item.txt\"), sep='\\t'\n",
    ")\n",
    "item_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download images from given urls. Please check `image_size` before going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(item_df: pd.DataFrame):\n",
    "    ids = item_df[ITEM.name]\n",
    "    urls = item_df['IMAGE_URL']\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for id_, url in tqdm.tqdm(zip(ids, urls), desc=\"Download images: \"):\n",
    "            if url:\n",
    "                executor.submit(\n",
    "                    download_from_url,\n",
    "                    url=url,\n",
    "                    root=image_folder,\n",
    "                    filename=f\"{id_}.jpg\",\n",
    "                    log=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download images: : 30380it [00:32, 937.29it/s] \n"
     ]
    }
   ],
   "source": [
    "download_images(item_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will encode visual modality first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(idx: int, processor):\n",
    "    has_image: bool = False\n",
    "    try:\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                image_folder, f\"{idx}.jpg\"\n",
    "            )\n",
    "        ).convert('RGB')\n",
    "        has_image = True\n",
    "    except FileNotFoundError:\n",
    "        image = Image.new('RGB', (224, 224))\n",
    "    return idx, has_image, processor(images=image, return_tensors='pt')['pixel_values'][0]\n",
    "\n",
    "def encode_visual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    model: str, model_dir: str,\n",
    "    num_workers: int = 4, batch_size: int = 128,\n",
    "):\n",
    "    from functools import partial\n",
    "    processor = AutoImageProcessor.from_pretrained(\n",
    "        os.path.join(model_dir, model), local_files_only=True\n",
    "    )\n",
    "\n",
    "    _process = partial(load_image, processor=processor)\n",
    "\n",
    "    datapipe = dp.iter.IterableWrapper(\n",
    "        range(len(item_df))\n",
    "    ).sharding_filter().map(\n",
    "        _process\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        datapipe, \n",
    "        num_workers=num_workers, batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = AutoModel.from_pretrained(\n",
    "        os.path.join(model_dir, model), local_files_only=True\n",
    "    ).to(device).eval()\n",
    "\n",
    "    vIndices = []\n",
    "    vMasks = []\n",
    "    vFeats = []\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        for (indices, has_images, images) in tqdm.tqdm(dataloader, desc=\"Visual batches: \"):\n",
    "            vIndices.append(indices)\n",
    "            vMasks.append(has_images)\n",
    "            outputs = encoder(pixel_values=images.to(device)).last_hidden_state\n",
    "            if outputs.ndim == 3:\n",
    "                # vit (Batch, Sequence, D)\n",
    "                outputs = outputs[:, 0]\n",
    "            else:\n",
    "                # resnet (Batch, D, K, K)\n",
    "                outputs = outputs.flatten(2).mean(-1)\n",
    "            vFeats.append(\n",
    "                outputs.detach().cpu()\n",
    "            )\n",
    "    vIndices = torch.cat(vIndices, dim=0)\n",
    "    vMasks = torch.cat(vMasks, dim=0)\n",
    "    vFeats = torch.cat(vFeats, dim=0).flatten(1) # (N, D)\n",
    "    vFeats = vFeats[vIndices.argsort()] # reindex\n",
    "    vMasks = vMasks[vIndices.argsort()]\n",
    "    assert vFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    vMasks = vMasks.to(vFeats.device)\n",
    "    mean = vFeats[vMasks].mean(dim=0, keepdim=True).repeat((vFeats.size(0), 1))\n",
    "    vMasks = vMasks.unsqueeze(-1).expand_as(vFeats)\n",
    "    vFeats = torch.where(vMasks, vFeats, mean)\n",
    "\n",
    "    export_pickle(\n",
    "        vFeats, os.path.join(\n",
    "            datadir, f\"visual_{model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return vFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visual batches: : 240it [03:45,  1.06it/s]                       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1044, -0.2491, -0.2130,  ..., -0.0333, -0.4862,  0.2023],\n",
       "        [-0.1021,  0.0347, -0.0537,  ..., -0.0105, -0.0494,  0.0819],\n",
       "        [-0.0634,  0.2227,  0.0578,  ..., -0.0093,  0.1451,  0.2042],\n",
       "        ...,\n",
       "        [-0.1225, -0.0066,  0.0214,  ..., -0.1641, -0.0435,  0.0231],\n",
       "        [ 0.0448, -0.2399,  0.0342,  ..., -0.1069,  0.0291, -0.0334],\n",
       "        [-0.0702,  0.3238, -0.1198,  ..., -0.3347,  0.2070, -0.0101]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_visual_modality(\n",
    "    item_df,\n",
    "    model=\"dino-resnet50\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_textual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    model: str, model_dir: str,\n",
    "    batch_size: int = 128\n",
    "):\n",
    "    sentences = item_df['TEXT']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, model),\n",
    "        device=device\n",
    "    ).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tFeats = encoder.encode(\n",
    "            sentences, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "    assert tFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    export_pickle(\n",
    "        tFeats, os.path.join(\n",
    "            datadir, f\"textual_{model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return tFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 238/238 [00:33<00:00,  7.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0750,  0.0625,  0.0448,  ..., -0.0347, -0.0460,  0.0341],\n",
       "        [-0.0064, -0.0411,  0.1195,  ..., -0.0291,  0.0006, -0.0331],\n",
       "        [-0.0876,  0.0186, -0.0125,  ..., -0.0514, -0.0609,  0.0226],\n",
       "        ...,\n",
       "        [-0.0577,  0.0095,  0.0716,  ..., -0.0412,  0.0741,  0.0501],\n",
       "        [-0.0054,  0.0916,  0.0795,  ...,  0.0172,  0.0215,  0.0051],\n",
       "        [-0.0679,  0.0423,  0.0517,  ..., -0.0599,  0.0046,  0.0219]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_textual_modality(\n",
    "    item_df,\n",
    "    model=\"all-MiniLM-L6-v2\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to extract features by CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_clip_textual_visual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    img_clip_model: str,\n",
    "    text_clip_model: str,\n",
    "    model_dir: str,\n",
    "    batch_size: int = 128\n",
    "):\n",
    "    images = []\n",
    "    vMasks = torch.ones((len(item_df,)))\n",
    "    for idx in range(len(item_df)):\n",
    "        try:\n",
    "            image = Image.open(\n",
    "                os.path.join(\n",
    "                    image_folder, f\"{idx}.jpg\"\n",
    "                )\n",
    "            ).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            vMasks[idx] = 0\n",
    "        images.append(image)\n",
    "    vMasks = vMasks.bool()\n",
    "\n",
    "    sentences = item_df['TEXT']\n",
    "\n",
    "    img_encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, img_clip_model),\n",
    "        device=torch.device('cuda:0')\n",
    "    ).eval()\n",
    "    text_encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, text_clip_model),\n",
    "        device=torch.device('cuda:1')\n",
    "    ).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vFeats = img_encoder.encode(\n",
    "            images,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "        tFeats = text_encoder.encode(\n",
    "            sentences, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "    assert vFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "    assert tFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    vMasks = vMasks.to(vFeats.device)\n",
    "    mean = vFeats[vMasks].mean(dim=0, keepdim=True).repeat((vFeats.size(0), 1))\n",
    "    vMasks = vMasks.unsqueeze(-1).expand_as(vFeats)\n",
    "    vFeats = torch.where(vMasks, vFeats, mean)\n",
    "\n",
    "    export_pickle(\n",
    "        vFeats, os.path.join(\n",
    "            datadir, f\"visual_{img_clip_model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    export_pickle(\n",
    "        tFeats, os.path.join(\n",
    "            datadir, f\"textual_{text_clip_model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return vFeats, tFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 238/238 [04:07<00:00,  1.04s/it]\n",
      "Batches: 100%|██████████| 238/238 [01:39<00:00,  2.39it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0367, -0.4885,  0.3980,  ..., -0.1649,  0.4623,  0.0621],\n",
       "         [-0.2082, -0.0440,  0.0904,  ...,  0.3867, -0.0183, -0.0969],\n",
       "         [-0.0832,  0.3915, -0.0901,  ...,  0.9376,  0.2346,  0.1945],\n",
       "         ...,\n",
       "         [-0.1495,  0.3978,  0.2217,  ...,  0.1837, -0.4292,  0.1551],\n",
       "         [-0.2783,  0.1620,  0.1765,  ...,  0.4337, -0.3054, -0.1246],\n",
       "         [-0.2874, -0.1172,  0.1805,  ...,  0.1124,  0.0043,  0.2117]]),\n",
       " tensor([[-0.0480,  0.1438, -0.0667,  ...,  0.0107,  0.0527, -0.1216],\n",
       "         [ 0.0211,  0.1272, -0.0593,  ...,  0.0686,  0.0270, -0.0941],\n",
       "         [-0.0333,  0.1581, -0.0986,  ...,  0.0823,  0.0271,  0.0128],\n",
       "         ...,\n",
       "         [ 0.0293,  0.1610, -0.0843,  ..., -0.0547, -0.0385, -0.0440],\n",
       "         [ 0.1392,  0.0145, -0.0654,  ..., -0.0097,  0.0556, -0.1122],\n",
       "         [ 0.0427,  0.1206, -0.1094,  ...,  0.0034, -0.0012, -0.0377]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_clip_textual_visual_modality(\n",
    "    item_df,\n",
    "    img_clip_model=\"clip-vit-b-32\",\n",
    "    text_clip_model=\"clip-vit-b-32-multilingual-v1\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('PyT22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "551ed5c3947f7ba38ed72fd7713d8d1c2fbde1c026852213575487749933a5e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
