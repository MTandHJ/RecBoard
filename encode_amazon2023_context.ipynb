{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Item Textual and Visual Modality Features for Amazon2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cxu/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Iterable\n",
    "\n",
    "import torch, tqdm, os\n",
    "import pandas as pd\n",
    "from freerec.data.tags import USER, ITEM\n",
    "from freerec.data.utils import download_from_url\n",
    "from freerec.utils import export_pickle\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torchdata.datapipes as dp\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download the models according to [[hf-mirror](https://hf-mirror.com/)]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset: str = \"Amazon2023Baby_554811_ROU\"\n",
    "datadir: str = f\"../data/Processed/{dataset}\"\n",
    "image_folder: str = os.path.join(datadir, \"item_images\", \"large\")\n",
    "model_cache_dir: str = \"../models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEM</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>IMAGE_URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Title: Baby Tracker® - Daily Childcare Journal...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41Bb6wf+qU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Title: Disney Mickey and Friends Baby Beginner...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/61tpXz7AAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Title: SoftPlay Fisher-Price Precious Planet C...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/516RIuPn3R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Title: Christian Art Gifts Girl Baby Book of M...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/41xViU1RwR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Title: Christian Art Gifts Boy Baby Book of Me...</td>\n",
       "      <td>https://m.media-amazon.com/images/I/415eCH3JG5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ITEM                                               TEXT  \\\n",
       "0     0  Title: Baby Tracker® - Daily Childcare Journal...   \n",
       "1     1  Title: Disney Mickey and Friends Baby Beginner...   \n",
       "2     2  Title: SoftPlay Fisher-Price Precious Planet C...   \n",
       "3     3  Title: Christian Art Gifts Girl Baby Book of M...   \n",
       "4     4  Title: Christian Art Gifts Boy Baby Book of Me...   \n",
       "\n",
       "                                           IMAGE_URL  \n",
       "0  https://m.media-amazon.com/images/I/41Bb6wf+qU...  \n",
       "1  https://m.media-amazon.com/images/I/61tpXz7AAT...  \n",
       "2  https://m.media-amazon.com/images/I/516RIuPn3R...  \n",
       "3  https://m.media-amazon.com/images/I/41xViU1RwR...  \n",
       "4  https://m.media-amazon.com/images/I/415eCH3JG5...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_df = pd.read_csv(\n",
    "    os.path.join(datadir, \"item.txt\"), sep='\\t'\n",
    ")\n",
    "item_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first download images from given urls. Please check `image_size` before going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(item_df: pd.DataFrame):\n",
    "    ids = item_df[ITEM.name]\n",
    "    urls = item_df['IMAGE_URL']\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for id_, url in tqdm.tqdm(zip(ids, urls), desc=\"Download images: \"):\n",
    "            if url:\n",
    "                executor.submit(\n",
    "                    download_from_url,\n",
    "                    url=url,\n",
    "                    root=image_folder,\n",
    "                    filename=f\"{id_}.jpg\",\n",
    "                    log=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Download images: : 30380it [00:32, 937.29it/s] \n"
     ]
    }
   ],
   "source": [
    "download_images(item_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will encode visual modality first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(idx: int, processor):\n",
    "    has_image: bool = False\n",
    "    try:\n",
    "        image = Image.open(\n",
    "            os.path.join(\n",
    "                image_folder, f\"{idx}.jpg\"\n",
    "            )\n",
    "        ).convert('RGB')\n",
    "        has_image = True\n",
    "    except FileNotFoundError:\n",
    "        image = Image.new('RGB', (224, 224))\n",
    "    return idx, has_image, processor(images=image, return_tensors='pt')['pixel_values'][0]\n",
    "\n",
    "def encode_visual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    model: str, model_dir: str,\n",
    "    num_workers: int = 4, batch_size: int = 128,\n",
    "):\n",
    "    from functools import partial\n",
    "    processor = AutoImageProcessor.from_pretrained(\n",
    "        os.path.join(model_dir, model), local_files_only=True\n",
    "    )\n",
    "\n",
    "    _process = partial(load_image, processor=processor)\n",
    "\n",
    "    datapipe = dp.iter.IterableWrapper(\n",
    "        range(len(item_df))\n",
    "    ).sharding_filter().map(\n",
    "        _process\n",
    "    )\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        datapipe, \n",
    "        num_workers=num_workers, batch_size=batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = AutoModel.from_pretrained(\n",
    "        os.path.join(model_dir, model), local_files_only=True\n",
    "    ).to(device).eval()\n",
    "\n",
    "    vIndices = []\n",
    "    vMasks = []\n",
    "    vFeats = []\n",
    "    with torch.no_grad():\n",
    "        encoder.eval()\n",
    "        for (indices, has_images, images) in tqdm.tqdm(dataloader, desc=\"Visual batches: \"):\n",
    "            vIndices.append(indices)\n",
    "            vMasks.append(has_images)\n",
    "            outputs = encoder(pixel_values=images.to(device)).last_hidden_state\n",
    "            if outputs.ndim == 3:\n",
    "                # vit (Batch, Sequence, D)\n",
    "                outputs = outputs[:, 0]\n",
    "            else:\n",
    "                # resnet (Batch, D, K, K)\n",
    "                outputs = outputs.flatten(2).mean(-1)\n",
    "            vFeats.append(\n",
    "                outputs.detach().cpu()\n",
    "            )\n",
    "    vIndices = torch.cat(vIndices, dim=0)\n",
    "    vMasks = torch.cat(vMasks, dim=0)\n",
    "    vFeats = torch.cat(vFeats, dim=0).flatten(1) # (N, D)\n",
    "    vFeats = vFeats[vIndices.argsort()] # reindex\n",
    "    vMasks = vMasks[vIndices.argsort()]\n",
    "    assert vFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    vMasks = vMasks.to(vFeats.device)\n",
    "    mean = vFeats[vMasks].mean(dim=0, keepdim=True).repeat((vFeats.size(0), 1))\n",
    "    vMasks = vMasks.unsqueeze(-1).expand_as(vFeats)\n",
    "    vFeats = torch.where(vMasks, vFeats, mean)\n",
    "\n",
    "    export_pickle(\n",
    "        vFeats, os.path.join(\n",
    "            datadir, f\"visual_{model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return vFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visual batches: : 240it [03:45,  1.06it/s]                       \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1044, -0.2491, -0.2130,  ..., -0.0333, -0.4862,  0.2023],\n",
       "        [-0.1021,  0.0347, -0.0537,  ..., -0.0105, -0.0494,  0.0819],\n",
       "        [-0.0634,  0.2227,  0.0578,  ..., -0.0093,  0.1451,  0.2042],\n",
       "        ...,\n",
       "        [-0.1225, -0.0066,  0.0214,  ..., -0.1641, -0.0435,  0.0231],\n",
       "        [ 0.0448, -0.2399,  0.0342,  ..., -0.1069,  0.0291, -0.0334],\n",
       "        [-0.0702,  0.3238, -0.1198,  ..., -0.3347,  0.2070, -0.0101]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_visual_modality(\n",
    "    item_df,\n",
    "    model=\"vit-base-16-224\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_textual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    model: str, model_dir: str,\n",
    "    batch_size: int = 128\n",
    "):\n",
    "    sentences = item_df['TEXT']\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, model),\n",
    "        device=device\n",
    "    ).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tFeats = encoder.encode(\n",
    "            sentences, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "    assert tFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    export_pickle(\n",
    "        tFeats, os.path.join(\n",
    "            datadir, f\"textual_{model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return tFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 259/259 [00:45<00:00,  5.66it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0899,  0.0039, -0.0233,  ..., -0.0025,  0.0305,  0.0443],\n",
       "        [ 0.0385, -0.0299,  0.0062,  ...,  0.0310, -0.0348,  0.0290],\n",
       "        [ 0.0183, -0.0791,  0.0461,  ..., -0.0465,  0.0215,  0.0571],\n",
       "        ...,\n",
       "        [-0.0214,  0.0251, -0.0389,  ..., -0.0851,  0.0558,  0.0507],\n",
       "        [ 0.0217,  0.0355, -0.0526,  ...,  0.0204,  0.0282,  0.0283],\n",
       "        [-0.0817, -0.0837, -0.0051,  ..., -0.0638,  0.0584,  0.0338]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_textual_modality(\n",
    "    item_df,\n",
    "    model=\"all-MiniLM-L6-v2\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is used to extract features by CLIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_clip_textual_visual_modality(\n",
    "    item_df: pd.DataFrame,\n",
    "    img_clip_model: str,\n",
    "    text_clip_model: str,\n",
    "    model_dir: str,\n",
    "    batch_size: int = 128\n",
    "):\n",
    "    images = []\n",
    "    vMasks = torch.ones((len(item_df,)))\n",
    "    for idx in range(len(item_df)):\n",
    "        try:\n",
    "            image = Image.open(\n",
    "                os.path.join(\n",
    "                    image_folder, f\"{idx}.jpg\"\n",
    "                )\n",
    "            ).convert('RGB')\n",
    "        except FileNotFoundError:\n",
    "            image = Image.new('RGB', (224, 224))\n",
    "            vMasks[idx] = 0\n",
    "        images.append(image)\n",
    "    vMasks = vMasks.bool()\n",
    "\n",
    "    sentences = item_df['TEXT']\n",
    "\n",
    "    img_encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, img_clip_model),\n",
    "        device=torch.device('cuda:0')\n",
    "    ).eval()\n",
    "    text_encoder = SentenceTransformer(\n",
    "        os.path.join(model_dir, text_clip_model),\n",
    "        device=torch.device('cuda:1')\n",
    "    ).eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vFeats = img_encoder.encode(\n",
    "            images,\n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "        tFeats = text_encoder.encode(\n",
    "            sentences, \n",
    "            convert_to_tensor=True,\n",
    "            batch_size=batch_size, show_progress_bar=True\n",
    "        ).cpu()\n",
    "    assert vFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "    assert tFeats.size(0) == len(item_df), f\"Unknown errors happen ...\"\n",
    "\n",
    "    vMasks = vMasks.to(vFeats.device)\n",
    "    mean = vFeats[vMasks].mean(dim=0, keepdim=True).repeat((vFeats.size(0), 1))\n",
    "    vMasks = vMasks.unsqueeze(-1).expand_as(vFeats)\n",
    "    vFeats = torch.where(vMasks, vFeats, mean)\n",
    "\n",
    "    export_pickle(\n",
    "        vFeats, os.path.join(\n",
    "            datadir, f\"visual_{img_clip_model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    export_pickle(\n",
    "        tFeats, os.path.join(\n",
    "            datadir, f\"textual_{text_clip_model}.pkl\"\n",
    "        )\n",
    "    )\n",
    "    return vFeats, tFeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 259/259 [04:28<00:00,  1.04s/it]\n",
      "Batches: 100%|██████████| 259/259 [02:19<00:00,  1.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.2293,  0.0023,  0.1084,  ...,  0.4313, -0.1574,  0.0206],\n",
       "         [-0.3903, -0.2919, -0.0827,  ...,  0.3836, -0.0038,  0.3730],\n",
       "         [-0.2293,  0.0023,  0.1084,  ...,  0.4313, -0.1574,  0.0206],\n",
       "         ...,\n",
       "         [-0.4276,  0.3432,  0.2271,  ...,  0.7927, -0.6071,  0.3222],\n",
       "         [-0.4272,  0.3286,  0.1283,  ..., -0.0385,  0.0927, -0.0549],\n",
       "         [-0.2324, -0.3409,  0.0483,  ..., -0.0207,  0.3209, -0.1621]]),\n",
       " tensor([[ 2.0746e-05,  7.1700e-02, -1.1730e-01,  ..., -6.6910e-02,\n",
       "           4.1709e-02, -3.1474e-02],\n",
       "         [-2.5776e-02,  1.2629e-01,  2.8502e-02,  ...,  1.1202e-01,\n",
       "          -1.3230e-02, -7.9388e-02],\n",
       "         [ 3.2196e-02,  1.1812e-01, -8.3679e-02,  ...,  2.1190e-02,\n",
       "           8.2094e-03, -2.7704e-02],\n",
       "         ...,\n",
       "         [ 5.8675e-03,  6.8395e-02, -1.3923e-01,  ..., -7.9439e-02,\n",
       "          -1.3898e-01, -1.5873e-01],\n",
       "         [ 1.8296e-02,  1.2071e-01, -9.2982e-02,  ...,  3.5933e-02,\n",
       "          -1.6972e-02, -9.6719e-03],\n",
       "         [-1.6469e-02,  1.0883e-01, -7.4217e-02,  ...,  6.3517e-02,\n",
       "           2.1785e-02, -2.5476e-02]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_clip_textual_visual_modality(\n",
    "    item_df,\n",
    "    img_clip_model=\"clip-vit-b-32\",\n",
    "    text_clip_model=\"clip-vit-b-32-multilingual-v1\",\n",
    "    model_dir=model_cache_dir\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.19 ('PyT22')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "551ed5c3947f7ba38ed72fd7713d8d1c2fbde1c026852213575487749933a5e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
